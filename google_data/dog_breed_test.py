# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from keras.applications.resnet50 import ResNet50

# Input data files are available in the "input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory

import os

# Any results you write to the current directory are saved as output.
#from skimage.transform import resize

import keras
import cv2
from keras.applications import inception_v3
from keras.applications.inception_v3 import InceptionV3
from keras.applications.inception_v3 import preprocess_input as inception_v3_preprocessor

from keras.preprocessing.image import img_to_array

from sklearn.model_selection import train_test_split

from keras.preprocessing.image import load_img

from keras.preprocessing.image import ImageDataGenerator

pd.options.mode.chained_assignment = None  # default='warn'


from keras.layers import Dense, GlobalAveragePooling2D
from keras.models import Model

from keras.optimizers import Adam
from keras.metrics import categorical_crossentropy

from keras.models import model_from_json

from tqdm import tqdm

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #

# Load Model & Weights:

with open('model_description.json', 'r') as f:
    json = f.read()
model = model_from_json(json)
model.load_weights('trained_weights.h5')


# Use the sample submission file to set up the test data - x_test
test_data = pd.read_csv('sample_submission.csv')

# Create the x_test
x_test = []
for i in tqdm(test_data['id'].values):
	try:
		img = cv2.imread('dog_breeds_random/{}.jpg'.format(i))
		x_test.append(cv2.resize(img, (299, 299)))
	except:
		print(i)

# Make it an array
x_test = np.array(x_test, np.float32) / 255.

# Predict x_test
predictions = model.predict(x_test, verbose=2)

# Get the labels of the top 120
train_dogs = pd.read_csv('labels.csv')
#Get the top 120 breeds which is what we use in this notebook
top_breeds = sorted(list(train_dogs['breed'].value_counts().head(120).index))
train_dogs = train_dogs[train_dogs['breed'].isin(top_breeds)]

target_labels = train_dogs['breed']
# One hot code the labels - need this for the model
one_hot = pd.get_dummies(target_labels, sparse = True)
one_hot_labels = np.asarray(one_hot)
# Set column names to those generated by the one-hot encoding earlier
col_names = one_hot.columns.values

# Create the submission data.
submission_results = pd.DataFrame(predictions, columns = col_names)

# Add the id as the first column
submission_results.insert(0, 'id', test_data['id'])

# Save the submission
submission_results.to_csv('submission.csv', index=False)
